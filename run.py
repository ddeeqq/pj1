"""
ì¤‘ê³ ì°¨ vs ì‹ ì°¨ ë¶„ì„ ì‹œìŠ¤í…œ - ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""
import os
import sys
import argparse
import logging
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/app_{datetime.now().strftime("%Y%m%d")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def run_streamlit():
    """Streamlit ì•± ì‹¤í–‰"""
    logger.info("ğŸš€ Streamlit ì•±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    os.system("streamlit run ui/streamlit_app.py")

def init_database():
    """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
    logger.info("ğŸ”§ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
    from database.database_schema import DatabaseManager
    
    db_manager = DatabaseManager()
    db_manager.create_database()
    db_manager.create_tables()
    db_manager.insert_sample_data()
    logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ!")

def crawl_all_data():
    """ëª¨ë“  ë°ì´í„° í¬ë¡¤ë§ ì‹¤í–‰"""
    logger.info("ğŸ•·ï¸ ì „ì²´ ë°ì´í„° í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # ê³µê³µë°ì´í„° ìˆ˜ì§‘
    try:
        from crawlers.public_data_crawler import PublicDataCrawler
        crawler = PublicDataCrawler()
        df = crawler.load_registration_data()
        if not df.empty:
            crawler.save_to_database(df)
            logger.info("âœ… ê³µê³µë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ ê³µê³µë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    # ì¤‘ê³ ì°¨ ê°€ê²© ìˆ˜ì§‘ (ì£¼ì˜: ì‹¤ì œ í¬ë¡¤ë§ ì‹œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
    try:
        from crawlers.encar_crawler import EncarCrawler
        from config.config import POPULAR_MODELS
        
        crawler = EncarCrawler()
        car_list = []
        for manufacturer, models in POPULAR_MODELS.items():
            for model in models[:2]:  # ê° ì œì¡°ì‚¬ë³„ 2ê°œ ëª¨ë¸ë§Œ í…ŒìŠ¤íŠ¸
                car_list.append({
                    'manufacturer': manufacturer,
                    'model_name': model
                })
        
        # crawler.crawl_and_save(car_list)  # ì‹¤ì œ í¬ë¡¤ë§ (ì£¼ì„ í•´ì œ ì‹œ ì‹¤í–‰)
        logger.info("âœ… ì¤‘ê³ ì°¨ ê°€ê²© í¬ë¡¤ë§ ì¤€ë¹„ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ ì¤‘ê³ ì°¨ ê°€ê²© ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    # ë¦¬ì½œ ì •ë³´ ìˆ˜ì§‘
    try:
        from crawlers.recall_crawler import RecallCrawler
        crawler = RecallCrawler()
        # crawler.crawl_and_save(car_list)  # ì‹¤ì œ í¬ë¡¤ë§ (ì£¼ì„ í•´ì œ ì‹œ ì‹¤í–‰)
        logger.info("âœ… ë¦¬ì½œ ì •ë³´ í¬ë¡¤ë§ ì¤€ë¹„ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ ë¦¬ì½œ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

def test_connection():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ” ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤...")
    
    try:
        from database.db_helper import db_helper
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        with db_helper.get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT VERSION()")
            version = cursor.fetchone()
            logger.info(f"âœ… MySQL ë²„ì „: {version}")
            
        # í…Œì´ë¸” í™•ì¸
        tables = db_helper.execute_query("SHOW TABLES")
        logger.info(f"âœ… í…Œì´ë¸” ìˆ˜: {len(tables)}ê°œ")
        for table in tables:
            table_name = list(table.values())[0]
            count = db_helper.execute_query(f"SELECT COUNT(*) as cnt FROM {table_name}")
            logger.info(f"   - {table_name}: {count[0]['cnt']}ê°œ ë ˆì½”ë“œ")
            
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
        return False
    
    return True

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ì¤‘ê³ ì°¨ vs ì‹ ì°¨ ë¶„ì„ ì‹œìŠ¤í…œ')
    parser.add_argument(
        'command',
        choices=['run', 'init', 'crawl', 'test'],
        help='ì‹¤í–‰í•  ëª…ë ¹ (run: ì•± ì‹¤í–‰, init: DB ì´ˆê¸°í™”, crawl: ë°ì´í„° ìˆ˜ì§‘, test: ì—°ê²° í…ŒìŠ¤íŠ¸)'
    )
    
    args = parser.parse_args()
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('logs', exist_ok=True)
    
    logger.info("=" * 50)
    logger.info("ğŸš— ì¤‘ê³ ì°¨ vs ì‹ ì°¨ ê°€ì„±ë¹„ ë¶„ì„ ì‹œìŠ¤í…œ")
    logger.info("=" * 50)
    
    if args.command == 'run':
        # ì—°ê²° í…ŒìŠ¤íŠ¸ í›„ ì•± ì‹¤í–‰
        if test_connection():
            run_streamlit()
        else:
            logger.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨. ë¨¼ì € 'python run.py init'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            
    elif args.command == 'init':
        init_database()
        
    elif args.command == 'crawl':
        crawl_all_data()
        
    elif args.command == 'test':
        if test_connection():
            logger.info("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        else:
            logger.error("âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")

if __name__ == "__main__":
    main()
