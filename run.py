"""
ì¤‘ê³ ì°¨ vs ì‹ ì°¨ ë¶„ì„ ì‹œìŠ¤í…œ - ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""
import os
import sys
import argparse
import logging
import json
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/app_{datetime.now().strftime("%Y%m%d")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def run_streamlit():
    """Streamlit ì•± ì‹¤í–‰"""
    logger.info("ğŸš€ Streamlit ì•±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    os.system("streamlit run ui/streamlit_app.py")

def init_database():
    """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
    logger.info("ğŸ”§ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
    from database.database_schema import DatabaseManager
    
    db_manager = DatabaseManager()
    db_manager.create_database()
    db_manager.create_tables()
    db_manager.insert_sample_data()
    logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ!")

def crawl_all_data():
    """ëª¨ë“  ë°ì´í„° í¬ë¡¤ë§ ì‹¤í–‰ (ë¦¬íŒ©í† ë§ëœ ë²„ì „)"""
    logger.info("ğŸ•·ï¸ ì „ì²´ ë°ì´í„° í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # ì„¤ì • ë¡œë“œ
    try:
        with open('config/scheduler_config.json', 'r', encoding='utf-8') as f:
            config = json.load(f)['crawling']
    except Exception as e:
        logger.error(f"ì„¤ì • íŒŒì¼ì„ ì½ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        return

    # 1. ê³µê³µë°ì´í„° ìˆ˜ì§‘
    try:
        from crawlers.public_data_crawler import PublicDataCrawler
        logger.info("--- ê³µê³µë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ---")
        pd_crawler = PublicDataCrawler(config.get('public_data', {}))
        df = pd_crawler.load_registration_data()
        if not df.empty:
            pd_crawler.save_to_database(df)
        logger.info("âœ… ê³µê³µë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ ê³µê³µë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    
    # 2. ì¤‘ê³ ì°¨ ê°€ê²© ìˆ˜ì§‘
    logger.warning("ì—”ì¹´ ì¤‘ê³ ì°¨ ê°€ê²© í¬ë¡¤ë§ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    response = input("ì¤‘ê³ ì°¨ ê°€ê²©ì„ í¬ë¡¤ë§í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
    if response == 'y':
        try:
            from crawlers.encar_crawler import EncarCrawler
            from config.config import POPULAR_MODELS
            logger.info("--- ì¤‘ê³ ì°¨ ê°€ê²© ìˆ˜ì§‘ ì‹œì‘ ---")
            
            encar_crawler = EncarCrawler(config.get('encar', {}))
            car_list = []
            # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ê° ì œì¡°ì‚¬ë³„ 1ê°œ ëª¨ë¸ë§Œ í¬ë¡¤ë§
            for manufacturer, models in POPULAR_MODELS.items():
                if models:
                    car_list.append({'manufacturer': manufacturer, 'model_name': models[0]})
            
            encar_crawler.crawl_and_save(car_list)
            logger.info("âœ… ì¤‘ê³ ì°¨ ê°€ê²© í¬ë¡¤ë§ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ì¤‘ê³ ì°¨ ê°€ê²© ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    # 3. ë¦¬ì½œ ì •ë³´ ìˆ˜ì§‘
    response = input("ë¦¬ì½œ ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
    if response == 'y':
        try:
            from crawlers.recall_crawler import RecallCrawler
            from database.db_helper import db_helper
            logger.info("--- ë¦¬ì½œ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘ ---")

            recall_crawler = RecallCrawler(config.get('recall', {}))
            models_df = db_helper.get_car_models()
            car_list = models_df.to_dict('records')

            recall_crawler.crawl_and_save(car_list)
            logger.info("âœ… ë¦¬ì½œ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ë¦¬ì½œ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

def test_connection():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ” ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤...")
    try:
        from database.db_helper import db_helper
        with db_helper.get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT VERSION()")
            version = cursor.fetchone()
            logger.info(f"âœ… MySQL ë²„ì „: {version[0]}")
        return True
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description='ì¤‘ê³ ì°¨ vs ì‹ ì°¨ ë¶„ì„ ì‹œìŠ¤í…œ')
    parser.add_argument(
        'command',
        choices=['run', 'init', 'crawl', 'test'],
        help='ì‹¤í–‰í•  ëª…ë ¹ (run: ì•± ì‹¤í–‰, init: DB ì´ˆê¸°í™”, crawl: ë°ì´í„° ìˆ˜ì§‘, test: ì—°ê²° í…ŒìŠ¤íŠ¸)'
    )
    args = parser.parse_args()
    
    os.makedirs('logs', exist_ok=True)
    logger.info(f"\n{'='*20} {args.command.upper()} ì‹œì‘ {'='*20}")
    
    if args.command == 'run':
        if test_connection():
            run_streamlit()
        else:
            logger.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨. ë¨¼ì € 'python run.py init'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    elif args.command == 'init':
        init_database()
    elif args.command == 'crawl':
        crawl_all_data()
    elif args.command == 'test':
        if test_connection():
            logger.info("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        else:
            logger.error("âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")

if __name__ == "__main__":
    main()
