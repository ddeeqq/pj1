"""
Automated Data Collection Scheduler (Integrated Version)
"""
import schedule
import time
import logging
import psutil
import json
from datetime import datetime, timedelta
import sys
import os
import smtplib
from email.mime.text import MimeText
from email.mime.multipart import MimeMultipart

sys.path.append(os.path.dirname(os.path.abspath(__file__)))
# from crawlers.encar_crawler import EncarCrawler
# from crawlers.recall_crawler import RecallCrawler
# from crawlers.public_data_crawler import PublicDataCrawler
from config.config import POPULAR_MODELS

# Logging configuration
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/scheduler_{datetime.now().strftime("%Y%m%d")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class EnhancedDataScheduler:
    def __init__(self, config_path='config/scheduler_config.json'):
        self.config = self._load_config(config_path)
        self.stats = {
            'total_runs': 0,
            'successful_runs': 0,
            'failed_runs': 0,
            'last_run': None,
            'performance_metrics': []
        }

        # ì„¤ì •ì— ê¸°ë°˜í•˜ì—¬ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        crawling_config = self.config.get('crawling', {})
        # self.encar_crawler = EncarCrawler(config=crawling_config.get('encar', {}))
        # self.recall_crawler = RecallCrawler(config=crawling_config.get('recall', {}))
        # self.public_crawler = PublicDataCrawler(config=crawling_config.get('public_data', {}))
        logger.info("WARNING: Crawlers are temporarily disabled.")
        
    def _load_config(self, config_path):
        """Load configuration file"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                logger.info(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ: {config_path}")
                return config
        except FileNotFoundError:
            logger.error(f"âŒ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
            sys.exit(1)
        except json.JSONDecodeError:
            logger.error(f"âŒ ì„¤ì • íŒŒì¼ì´ ìœ íš¨í•œ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤: {config_path}")
            sys.exit(1)
        except Exception as e:
            logger.error(f"âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            sys.exit(1)

    def check_system_resources(self):
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸"""
        try:
            limits = self.config.get('resource_limits', {})
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            resources = {
                'cpu_percent': cpu_percent,
                'memory_percent': memory.percent,
                'memory_available': memory.available // (1024**3),  # GB
                'disk_percent': disk.percent,
                'disk_free': disk.free // (1024**3)  # GB
            }
            
            logger.info(f"ğŸ’» ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤: CPU {cpu_percent}%, RAM {memory.percent}%, Disk {disk.percent}%")
            
            # ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ê²½ê³  (ì„¤ì • íŒŒì¼ ê¸°ì¤€)
            if cpu_percent > limits.get('max_cpu_percent', 80):
                logger.warning(f"High CPU usage: {cpu_percent}%")
            if memory.percent > limits.get('max_memory_percent', 85):
                logger.warning(f"High memory usage: {memory.percent}%")
            if disk.percent > limits.get('max_disk_percent', 90):
                logger.warning(f"High disk usage: {disk.percent}%")
                
            return resources
            
        except Exception as e:
            logger.error(f"ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸ ì‹¤íŒ¨: {e}")
            return None
    
    def send_email_notification(self, subject, message):
        """ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡"""
        email_conf = self.config.get('email', {})
        if not email_conf.get('enabled') or not email_conf.get('recipients'):
            return
            
        try:
            msg = MimeMultipart()
            msg['From'] = email_conf['email']
            msg['To'] = ', '.join(email_conf['recipients'])
            msg['Subject'] = f"[ì°¨ëŸ‰ë¶„ì„ì‹œìŠ¤í…œ] {subject}"
            
            msg.attach(MimeText(message, 'plain', 'utf-8'))
            
            server = smtplib.SMTP(email_conf['smtp_server'], email_conf['smtp_port'])
            server.starttls()
            server.login(email_conf['email'], email_conf['password'])
            text = msg.as_string()
            server.sendmail(email_conf['email'], email_conf['recipients'], text)
            server.quit()
            
            logger.info(" ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")
    
    def retry_with_backoff(self, func, *args, **kwargs):
        """ì§€ìˆ˜ ë°±ì˜¤í”„ë¥¼ ì‚¬ìš©í•œ ì¬ì‹œë„ ë¡œì§"""
        max_retries = self.config.get('scheduler', {}).get('max_retries', 3)
        for attempt in range(max_retries):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                wait_time = 2 ** attempt
                logger.warning(f" ì‘ì—… ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                
                if attempt < max_retries - 1:
                    logger.info(f" {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
                else:
                    logger.error(f" ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ì‘ì—… ì‹¤íŒ¨: {func.__name__}")
                    raise
    
    def validate_collected_data(self, source, records_collected):
        """ìˆ˜ì§‘ëœ ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        alerts_conf = self.config.get('alerts', {})
        if not self.config.get('scheduler', {}).get('enable_data_validation', True):
            return

        try:
            from database.db_helper import db_helper
            
            query = "SELECT AVG(records_collected) as avg_records, COUNT(*) as log_count FROM CrawlingLog WHERE source = %s AND status = 'ì™„ë£Œ' AND started_at >= DATE_SUB(NOW(), INTERVAL 7 DAY)"
            result = db_helper.execute_query(query, [source])
            
            if result and result[0]['log_count'] > 0:
                avg_records = result[0]['avg_records'] or 0
                threshold = alerts_conf.get('data_collection_drop_threshold', 0.5)
                
                if records_collected < avg_records * threshold:
                    logger.warning(f"âš ï¸ ë°ì´í„° ìˆ˜ì§‘ëŸ‰ ê¸‰ê°: {source} (í‰ê· : {avg_records:.0f}, í˜„ì¬: {records_collected})")
                    self.send_email_notification(
                        f"ë°ì´í„° ìˆ˜ì§‘ëŸ‰ ê¸‰ê° ì•Œë¦¼",
                        f"ì†ŒìŠ¤: {source}\ní‰ê·  ìˆ˜ì§‘ëŸ‰: {avg_records:.0f}\ní˜„ì¬ ìˆ˜ì§‘ëŸ‰: {records_collected}\n\ní™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                elif records_collected > avg_records * 2:
                    logger.info(f" ë°ì´í„° ìˆ˜ì§‘ëŸ‰ ì¦ê°€: {source} (í‰ê· : {avg_records:.0f}, í˜„ì¬: {records_collected})")
                    
        except Exception as e:
            logger.error(f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
    
    def backup_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…"""
        try:
            from database.db_helper import db_helper
            import pandas as pd
            
            backup_dir = f"data/backup/{datetime.now().strftime('%Y%m%d')}"
            os.makedirs(backup_dir, exist_ok=True)
            
            important_tables = ['CarModel', 'UsedCarPrice', 'NewCarPrice', 'RegistrationStats', 'RecallInfo']
            
            for table in important_tables:
                try:
                    df = db_helper.fetch_dataframe(f"SELECT * FROM {table}")
                    if not df.empty:
                        backup_file = os.path.join(backup_dir, f"{table}_{datetime.now().strftime('%H%M')}.csv")
                        df.to_csv(backup_file, index=False, encoding='utf-8')
                        logger.info(f"ğŸ“ {table} ë°±ì—… ì™„ë£Œ ({len(df)}ê±´)")
                except Exception as e:
                    logger.error(f"í…Œì´ë¸” {table} ë°±ì—… ì‹¤íŒ¨: {e}")
            logger.info(f" ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì™„ë£Œ: {backup_dir}")
            
        except Exception as e:
            logger.error(f"ë°±ì—… ì‹¤íŒ¨: {e}")

    def daily_price_update(self):
        """ì¼ì¼ ê°€ê²© ì—…ë°ì´íŠ¸"""
        start_time = datetime.now()
        logger.info("ğŸŒ™ ì¼ì¼ ê°€ê²© ì—…ë°ì´íŠ¸ ì‹œì‘...")
        
        resources = self.check_system_resources()
        
        try:
            car_list = [{'manufacturer': m, 'model_name': models[0]} for m, models in POPULAR_MODELS.items()]
            
            
            
            # self.retry_with_backoff(lambda: self.encar_crawler.crawl_and_save(car_list))
            logger.info("âš ï¸  ì—”ì¹´ í¬ë¡¤ë§ì´ ì¼ì‹œì ìœ¼ë¡œ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            duration = (datetime.now() - start_time).total_seconds()
            
            from database.db_helper import db_helper
            recent_log = db_helper.execute_query("SELECT records_collected FROM CrawlingLog WHERE source = 'encar' ORDER BY started_at DESC LIMIT 1")
            records_collected = recent_log[0]['records_collected'] if recent_log else 0
            self.validate_collected_data('encar', records_collected)
            
            self.stats['successful_runs'] += 1
            if self.config.get('scheduler', {}).get('enable_performance_monitoring', True):
                self.stats['performance_metrics'].append({
                    'task': 'daily_price_update', 'duration': duration, 'records': records_collected,
                    'timestamp': start_time.isoformat(), 'resources': resources
                })
            
            logger.info(f"âœ… ì¼ì¼ ê°€ê²© ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {duration:.1f}ì´ˆ)")
            
        except Exception as e:
            self.stats['failed_runs'] += 1
            logger.error(f"âŒ ê°€ê²© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            if self.config.get('email', {}).get('send_error_alerts', True):
                self.send_email_notification("ê°€ê²© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨", f"ì‹œê°„: {start_time}\nì˜¤ë¥˜: {str(e)}\n\ní™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        finally:
            self.stats['total_runs'] += 1
            self.stats['last_run'] = datetime.now().isoformat()

    def weekly_recall_update(self):
        """ì£¼ê°„ ë¦¬ì½œ ì •ë³´ ì—…ë°ì´íŠ¸"""
        start_time = datetime.now()
        logger.info(" ì£¼ê°„ ë¦¬ì½œ ì •ë³´ ì—…ë°ì´íŠ¸ ì‹œì‘...")
        
        try:
            from database.db_helper import db_helper
            models_df = db_helper.get_car_models()
            car_list = [{'manufacturer': r['manufacturer'], 'model_name': r['model_name']} for i, r in models_df.iterrows()]
            
            # self.retry_with_backoff(lambda: self.recall_crawler.crawl_and_save(car_list))
            logger.info("âš ï¸  ë¦¬ì½œ í¬ë¡¤ë§ì´ ì¼ì‹œì ìœ¼ë¡œ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            keywords = self.config.get('alerts', {}).get('critical_recall_keywords', ['í™”ì¬', 'ë¸Œë ˆì´í¬'])
            placeholders = ','.join(['%s'] * len(keywords))
            query = f"SELECT COUNT(*) as count FROM RecallInfo WHERE ({' OR '.join(['contents LIKE %s'] * len(keywords))}) AND recall_date >= DATE_SUB(CURDATE(), INTERVAL 7 DAY)"
            params = [f'%{keyword}%' for keyword in keywords]
            
            critical_recalls = db_helper.execute_query(query, params)
            
            if critical_recalls and critical_recalls[0]['count'] > 0:
                count = critical_recalls[0]['count']
                logger.warning(f" ì£¼ì˜: ìµœê·¼ ì¼ì£¼ì¼ê°„ ì‹¬ê°í•œ ë¦¬ì½œ {count}ê±´ ë°œìƒ")
                if self.config.get('alerts', {}).get('notify_on_new_critical_recalls', True):
                    self.send_email_notification(f"ì‹¬ê°í•œ ë¦¬ì½œ {count}ê±´ ë°œìƒ", f"ìµœê·¼ ì¼ì£¼ì¼ê°„ ì‹¬ê°ë„ê°€ ë†’ì€ ë¦¬ì½œì´ {count}ê±´ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\nì‹œìŠ¤í…œì—ì„œ í™•ì¸í•´ì£¼ì„¸ìš”.")
            
            duration = (datetime.now() - start_time).total_seconds()
            logger.info(f"âœ… ë¦¬ì½œ ì •ë³´ ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {duration:.1f}ì´ˆ)")
            
        except Exception as e:
            logger.error(f"âŒ ë¦¬ì½œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    def monthly_registration_update(self):
        """ì›”ê°„ ë“±ë¡ í˜„í™© ì—…ë°ì´íŠ¸"""
        logger.info(" ì›”ê°„ ë“±ë¡ í˜„í™© ì—…ë°ì´íŠ¸ ì‹œì‘...")
        try:
            def registration_task():
                # df = self.public_crawler.load_registration_data()
                # if not df.empty:
                #     self.public_crawler.save_to_database(df)
                #     return len(df)
                logger.info("âš ï¸  ê³µê³µë°ì´í„° í¬ë¡¤ë§ì´ ì¼ì‹œì ìœ¼ë¡œ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return 0
            
            records_count = self.retry_with_backoff(registration_task)
            if records_count > 0:
                logger.info(f"âœ… ë“±ë¡ í˜„í™© ì—…ë°ì´íŠ¸ ì™„ë£Œ ({records_count}ê±´)")
            else:
                logger.warning("ë“±ë¡ í˜„í™© ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"âŒ ë“±ë¡ í˜„í™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    def enhanced_health_check(self):
        """í–¥ìƒëœ ì‹œìŠ¤í…œ ìƒíƒœ ì²´í¬"""
        logger.info(" ì‹œìŠ¤í…œ ì¢…í•© ìƒíƒœ ì²´í¬...")
        health_status = {'database': False, 'disk_space': False, 'memory': False, 'recent_errors': 0, 'data_freshness': False}
        
        try:
            from database.db_helper import db_helper
            with db_helper.get_db_connection():
                health_status['database'] = True
            logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ìƒ")
            
            resources = self.check_system_resources()
            if resources:
                limits = self.config.get('resource_limits', {})
                health_status['disk_space'] = resources['disk_percent'] < limits.get('max_disk_percent', 90)
                health_status['memory'] = resources['memory_percent'] < limits.get('max_memory_percent', 85)
            
            error_logs = db_helper.execute_query("SELECT COUNT(*) as error_count FROM CrawlingLog WHERE status = 'ì‹¤íŒ¨' AND started_at >= DATE_SUB(NOW(), INTERVAL 24 HOUR)")
            if error_logs:
                health_status['recent_errors'] = error_logs[0]['error_count']
                if health_status['recent_errors'] > 0:
                    logger.warning(f"âš ï¸ ìµœê·¼ 24ì‹œê°„ ì˜¤ë¥˜: {health_status['recent_errors']}ê±´")
            
            latest_data = db_helper.execute_query("SELECT MAX(collected_date) as latest_date FROM UsedCarPrice")
            if latest_data and latest_data[0]['latest_date']:
                days_old = (datetime.now().date() - latest_data[0]['latest_date']).days
                health_status['data_freshness'] = days_old <= 3
                if days_old > 7:
                    logger.warning(f"âš ï¸ ê°€ê²© ë°ì´í„°ê°€ {days_old}ì¼ ì „ ê²ƒì…ë‹ˆë‹¤.")
            
            passed_checks = sum(v for k, v in health_status.items() if k != 'recent_errors' and v)
            total_checks = len(health_status) - 1
            health_score = (passed_checks / total_checks) * 100 if total_checks > 0 else 0
            logger.info(f"ğŸ¥ ì‹œìŠ¤í…œ ê±´ê°•ë„: {health_score:.0f}% ({passed_checks}/{total_checks})")
            
            if health_score < self.config.get('alerts', {}).get('system_health_threshold', 70):
                self.send_email_notification(f"ì‹œìŠ¤í…œ ê±´ê°•ë„ ì €í•˜ ({health_score:.0f}%)", f"ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n\nìƒíƒœ ì •ë³´:\n{json.dumps(health_status, indent=2, ensure_ascii=False)}")
            
        except Exception as e:
            logger.error(f"âŒ ê±´ê°• ìƒíƒœ ì²´í¬ ì‹¤íŒ¨: {e}")

    def cleanup_old_data_enhanced(self):
        """í–¥ìƒëœ ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬"""
        logger.info("ğŸ§¹ ë°ì´í„° ì •ë¦¬ ì‹œì‘...")
        try:
            from database.db_helper import db_helper
            retention_conf = self.config.get('data_retention', {})
            
            price_days = retention_conf.get('price_data_days', 30)
            deleted_prices = db_helper.execute_query(f"DELETE FROM UsedCarPrice WHERE collected_date < DATE_SUB(CURDATE(), INTERVAL {price_days} DAY)", fetch=False)
            logger.info(f"âœ… {price_days}ì¼ ì´ìƒëœ ê°€ê²© ë°ì´í„° {deleted_prices}ê±´ ì •ë¦¬")
            
            log_days = retention_conf.get('log_data_days', 90)
            deleted_logs = db_helper.execute_query(f"DELETE FROM CrawlingLog WHERE started_at < DATE_SUB(NOW(), INTERVAL {log_days} DAY)", fetch=False)
            logger.info(f"âœ… {log_days}ì¼ ì´ìƒëœ ë¡œê·¸ {deleted_logs}ê±´ ì •ë¦¬")
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨: {e}")

    def generate_daily_report(self):
        """ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not self.config.get('email', {}).get('send_daily_reports', True):
            return
        logger.info("ğŸ“„ ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        # (ë¦¬í¬íŠ¸ ìƒì„± ë¡œì§ì€ ê¸°ì¡´ê³¼ ìœ ì‚¬í•˜ê²Œ ìœ ì§€)
        # ...
        
    def setup_schedule(self):
        """ìŠ¤ì¼€ì¤„ ì„¤ì •"""
        schedule.every().day.at("03:00").do(self.daily_price_update)
        if self.config.get('email', {}).get('send_daily_reports', True):
            schedule.every().day.at("23:30").do(self.generate_daily_report)
        
        schedule.every().monday.at("04:00").do(self.weekly_recall_update)
        schedule.every().sunday.at("02:00").do(self.cleanup_old_data_enhanced)
        schedule.every().sunday.at("01:00").do(self.backup_database)
        
        schedule.every().day.at("05:00").do(self._check_monthly_task)
        schedule.every().hour.do(self.enhanced_health_check)
        
        logger.info("âœ… ìŠ¤ì¼€ì¤„ ì„¤ì • ì™„ë£Œ")

    def _check_monthly_task(self):
        if datetime.now().day == 1:
            self.monthly_registration_update()
    
    def run(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰"""
        logger.info("ğŸš€ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘...")
        self.enhanced_health_check()
        self.setup_schedule()
        
        logger.info(" ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. Ctrl+Cë¡œ ì¢…ë£Œí•˜ì„¸ìš”.")
        try:
            while True:
                schedule.run_pending()
                delay = self.config.get('scheduler', {}).get('delay_between_tasks', 60)
                time.sleep(delay)
        except KeyboardInterrupt:
            logger.info("ğŸ›‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ...")
            # if self.encar_crawler:
            #     self.encar_crawler.close_driver()
            logger.info(" ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì •ìƒì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬')
    parser.add_argument('--config', default='config/scheduler_config.json', help='ì„¤ì • íŒŒì¼ ê²½ë¡œ (JSON)')
    parser.add_argument('--task', choices=['price', 'recall', 'registration', 'health', 'cleanup', 'report', 'backup'], help='íŠ¹ì • ì‘ì—…ë§Œ ì‹¤í–‰')
    
    args = parser.parse_args()
    
    scheduler = EnhancedDataScheduler(config_path=args.config)
    
    if args.task:
        logger.info(f" ë‹¨ì¼ ì‘ì—… ì‹¤í–‰: {args.task}")
        tasks = {
            'price': scheduler.daily_price_update,
            'recall': scheduler.weekly_recall_update,
            'registration': scheduler.monthly_registration_update,
            'health': scheduler.enhanced_health_check,
            'cleanup': scheduler.cleanup_old_data_enhanced,
            'report': scheduler.generate_daily_report,
            'backup': scheduler.backup_database,
        }
        task_func = tasks.get(args.task)
        if task_func:
            task_func()
        else:
            logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ì‘ì—…ì…ë‹ˆë‹¤: {args.task}")
    else:
        scheduler.run()
