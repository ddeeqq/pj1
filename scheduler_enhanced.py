"""
ë°ì´í„° ìˆ˜ì§‘ ìë™ ìŠ¤ì¼€ì¤„ëŸ¬ (Enhanced Version)
"""
import schedule
import time
import logging
import psutil
import json
from datetime import datetime, timedelta
import sys
import os
import smtplib
from email.mime.text import MimeText
from email.mime.multipart import MimeMultipart

sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from crawlers.encar_crawler import EncarCrawler
from crawlers.recall_crawler import RecallCrawler
from crawlers.public_data_crawler import PublicDataCrawler
from config.config import POPULAR_MODELS

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/scheduler_{datetime.now().strftime("%Y%m%d")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class EnhancedDataScheduler:
    def __init__(self):
        self.encar_crawler = None
        self.recall_crawler = RecallCrawler()
        self.public_crawler = PublicDataCrawler()
        self.stats = {
            'total_runs': 0,
            'successful_runs': 0,
            'failed_runs': 0,
            'last_run': None,
            'performance_metrics': []
        }
        self.max_retries = 3
        self.email_config = {
            'enabled': False,  # ì´ë©”ì¼ ì•Œë¦¼ ì‚¬ìš© ì—¬ë¶€
            'smtp_server': 'smtp.gmail.com',
            'smtp_port': 587,
            'email': '',  # ë°œì†¡ì ì´ë©”ì¼
            'password': '',  # ì•± ë¹„ë°€ë²ˆí˜¸
            'recipients': []  # ìˆ˜ì‹ ì ëª©ë¡
        }
        
    def check_system_resources(self):
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸"""
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            resources = {
                'cpu_percent': cpu_percent,
                'memory_percent': memory.percent,
                'memory_available': memory.available // (1024**3),  # GB
                'disk_percent': disk.percent,
                'disk_free': disk.free // (1024**3)  # GB
            }
            
            logger.info(f"ğŸ’» ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤: CPU {cpu_percent}%, RAM {memory.percent}%, Disk {disk.percent}%")
            
            # ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ê²½ê³ 
            if cpu_percent > 80:
                logger.warning(f"âš ï¸ CPU ì‚¬ìš©ë¥  ë†’ìŒ: {cpu_percent}%")
            if memory.percent > 85:
                logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë†’ìŒ: {memory.percent}%")
            if disk.percent > 90:
                logger.warning(f"âš ï¸ ë””ìŠ¤í¬ ì‚¬ìš©ë¥  ë†’ìŒ: {disk.percent}%")
                
            return resources
            
        except Exception as e:
            logger.error(f"ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸ ì‹¤íŒ¨: {e}")
            return None
    
    def send_email_notification(self, subject, message):
        """ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡"""
        if not self.email_config['enabled'] or not self.email_config['recipients']:
            return
            
        try:
            msg = MimeMultipart()
            msg['From'] = self.email_config['email']
            msg['To'] = ', '.join(self.email_config['recipients'])
            msg['Subject'] = f"[ì°¨ëŸ‰ë¶„ì„ì‹œìŠ¤í…œ] {subject}"
            
            msg.attach(MimeText(message, 'plain', 'utf-8'))
            
            server = smtplib.SMTP(self.email_config['smtp_server'], self.email_config['smtp_port'])
            server.starttls()
            server.login(self.email_config['email'], self.email_config['password'])
            text = msg.as_string()
            server.sendmail(self.email_config['email'], self.email_config['recipients'], text)
            server.quit()
            
            logger.info("ğŸ“§ ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {e}")
    
    def retry_with_backoff(self, func, *args, **kwargs):
        """ì§€ìˆ˜ ë°±ì˜¤í”„ë¥¼ ì‚¬ìš©í•œ ì¬ì‹œë„ ë¡œì§"""
        for attempt in range(self.max_retries):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                wait_time = 2 ** attempt  # 1, 2, 4ì´ˆ
                logger.warning(f"â° ì‘ì—… ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{self.max_retries}): {e}")
                
                if attempt < self.max_retries - 1:
                    logger.info(f"ğŸ’¤ {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ì‘ì—… ì‹¤íŒ¨: {func.__name__}")
                    raise
    
    def validate_collected_data(self, source, records_collected):
        """ìˆ˜ì§‘ëœ ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        try:
            from database.db_helper import db_helper
            
            # ìµœê·¼ ìˆ˜ì§‘ ë°ì´í„°ì™€ ë¹„êµ
            query = """
            SELECT AVG(records_collected) as avg_records, COUNT(*) as log_count
            FROM CrawlingLog 
            WHERE source = %s AND status = 'ì™„ë£Œ' 
            AND started_at >= DATE_SUB(NOW(), INTERVAL 7 DAY)
            """
            
            result = db_helper.execute_query(query, [source])
            
            if result and result[0]['log_count'] > 0:
                avg_records = result[0]['avg_records'] or 0
                
                # í‰ê·  ëŒ€ë¹„ 50% ì´í•˜ë©´ ê²½ê³ 
                if records_collected < avg_records * 0.5:
                    logger.warning(f"âš ï¸ ë°ì´í„° ìˆ˜ì§‘ëŸ‰ ê¸‰ê°: {source} (í‰ê· : {avg_records:.0f}, í˜„ì¬: {records_collected})")
                    self.send_email_notification(
                        f"ë°ì´í„° ìˆ˜ì§‘ëŸ‰ ê¸‰ê° ì•Œë¦¼",
                        f"ì†ŒìŠ¤: {source}\ní‰ê·  ìˆ˜ì§‘ëŸ‰: {avg_records:.0f}\ní˜„ì¬ ìˆ˜ì§‘ëŸ‰: {records_collected}\n\ní™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
                    )
                elif records_collected > avg_records * 2:
                    logger.info(f"ğŸ“ˆ ë°ì´í„° ìˆ˜ì§‘ëŸ‰ ì¦ê°€: {source} (í‰ê· : {avg_records:.0f}, í˜„ì¬: {records_collected})")
                    
        except Exception as e:
            logger.error(f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
    
    def backup_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… (ì£¼ìš” í…Œì´ë¸”ë§Œ)"""
        try:
            from database.db_helper import db_helper
            import pandas as pd
            
            backup_dir = f"data/backup/{datetime.now().strftime('%Y%m%d')}"
            os.makedirs(backup_dir, exist_ok=True)
            
            # ì£¼ìš” í…Œì´ë¸” ë°±ì—…
            important_tables = [
                'CarModel', 'UsedCarPrice', 'NewCarPrice', 
                'RegistrationStats', 'RecallInfo'
            ]
            
            for table in important_tables:
                try:
                    query = f"SELECT * FROM {table}"
                    df = db_helper.fetch_dataframe(query)
                    
                    if not df.empty:
                        backup_file = os.path.join(backup_dir, f"{table}_{datetime.now().strftime('%H%M')}.csv")
                        df.to_csv(backup_file, index=False, encoding='utf-8')
                        logger.info(f"ğŸ“ {table} ë°±ì—… ì™„ë£Œ ({len(df)}ê±´)")
                        
                except Exception as e:
                    logger.error(f"í…Œì´ë¸” {table} ë°±ì—… ì‹¤íŒ¨: {e}")
                    
            logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì™„ë£Œ: {backup_dir}")
            
        except Exception as e:
            logger.error(f"ë°±ì—… ì‹¤íŒ¨: {e}")
    
    def daily_price_update(self):
        """ì¼ì¼ ê°€ê²© ì—…ë°ì´íŠ¸ (ê°œì„ ëœ ë²„ì „)"""
        start_time = datetime.now()
        logger.info("ğŸŒ™ ì¼ì¼ ê°€ê²© ì—…ë°ì´íŠ¸ ì‹œì‘...")
        
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸
        resources = self.check_system_resources()
        
        try:
            # ì¸ê¸° ëª¨ë¸ Top 10ë§Œ ì—…ë°ì´íŠ¸
            car_list = []
            for manufacturer, models in POPULAR_MODELS.items():
                for model in models[:1]:  # ê° ì œì¡°ì‚¬ë³„ 1ê°œì”©ë§Œ
                    car_list.append({
                        'manufacturer': manufacturer,
                        'model_name': model
                    })
            
            # ì—”ì¹´ í¬ë¡¤ëŸ¬ëŠ” í•„ìš”ì‹œì—ë§Œ ì´ˆê¸°í™” (ë¦¬ì†ŒìŠ¤ ì ˆì•½)
            if not self.encar_crawler:
                self.encar_crawler = EncarCrawler()
            
            # ì¬ì‹œë„ ë¡œì§ ì ìš©
            def crawl_task():
                return self.encar_crawler.crawl_and_save(car_list)
            
            self.retry_with_backoff(crawl_task)
            
            # ì„±ëŠ¥ í†µê³„ ê¸°ë¡
            duration = (datetime.now() - start_time).total_seconds()
            
            # ìˆ˜ì§‘ ë°ì´í„° ê²€ì¦
            from database.db_helper import db_helper
            recent_log = db_helper.execute_query("""
                SELECT records_collected FROM CrawlingLog 
                WHERE source = 'encar' 
                ORDER BY started_at DESC LIMIT 1
            """)
            
            records_collected = recent_log[0]['records_collected'] if recent_log else 0
            self.validate_collected_data('encar', records_collected)
            
            self.stats['successful_runs'] += 1
            self.stats['performance_metrics'].append({
                'task': 'daily_price_update',
                'duration': duration,
                'records': records_collected,
                'timestamp': start_time.isoformat(),
                'resources': resources
            })
            
            logger.info(f"âœ… ì¼ì¼ ê°€ê²© ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {duration:.1f}ì´ˆ)")
            
        except Exception as e:
            self.stats['failed_runs'] += 1
            logger.error(f"âŒ ê°€ê²© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            
            # ì‹¤íŒ¨ ì•Œë¦¼
            self.send_email_notification(
                "ê°€ê²© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨",
                f"ì‹œê°„: {start_time}\nì˜¤ë¥˜: {str(e)}\n\ní™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
            )
            
        finally:
            self.stats['total_runs'] += 1
            self.stats['last_run'] = datetime.now().isoformat()
    
    def weekly_recall_update(self):
        """ì£¼ê°„ ë¦¬ì½œ ì •ë³´ ì—…ë°ì´íŠ¸ (ê°œì„ ëœ ë²„ì „)"""
        start_time = datetime.now()
        logger.info("ğŸ“… ì£¼ê°„ ë¦¬ì½œ ì •ë³´ ì—…ë°ì´íŠ¸ ì‹œì‘...")
        
        try:
            from database.db_helper import db_helper
            
            models_df = db_helper.get_car_models()
            car_list = []
            
            for _, model in models_df.iterrows():
                car_list.append({
                    'manufacturer': model['manufacturer'],
                    'model_name': model['model_name']
                })
            
            def recall_task():
                return self.recall_crawler.crawl_and_save(car_list)
            
            self.retry_with_backoff(recall_task)
            
            # ë¦¬ì½œ í†µê³„ í™•ì¸
            critical_recalls = db_helper.execute_query("""
                SELECT COUNT(*) as count FROM RecallInfo 
                WHERE severity_level IN ('ì‹¬ê°', 'ë§¤ìš°ì‹¬ê°') 
                AND recall_date >= DATE_SUB(CURDATE(), INTERVAL 7 DAY)
            """)
            
            if critical_recalls and critical_recalls[0]['count'] > 0:
                count = critical_recalls[0]['count']
                logger.warning(f"ğŸš¨ ì£¼ì˜: ìµœê·¼ ì¼ì£¼ì¼ê°„ ì‹¬ê°í•œ ë¦¬ì½œ {count}ê±´ ë°œìƒ")
                self.send_email_notification(
                    f"ì‹¬ê°í•œ ë¦¬ì½œ {count}ê±´ ë°œìƒ",
                    f"ìµœê·¼ ì¼ì£¼ì¼ê°„ ì‹¬ê°ë„ê°€ ë†’ì€ ë¦¬ì½œì´ {count}ê±´ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\nì‹œìŠ¤í…œì—ì„œ í™•ì¸í•´ì£¼ì„¸ìš”."
                )
            
            duration = (datetime.now() - start_time).total_seconds()
            logger.info(f"âœ… ë¦¬ì½œ ì •ë³´ ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {duration:.1f}ì´ˆ)")
            
        except Exception as e:
            logger.error(f"âŒ ë¦¬ì½œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def monthly_registration_update(self):
        """ì›”ê°„ ë“±ë¡ í˜„í™© ì—…ë°ì´íŠ¸ (ê°œì„ ëœ ë²„ì „)"""
        start_time = datetime.now()
        logger.info("ğŸ“Š ì›”ê°„ ë“±ë¡ í˜„í™© ì—…ë°ì´íŠ¸ ì‹œì‘...")
        
        try:
            def registration_task():
                df = self.public_crawler.load_registration_data()
                if not df.empty:
                    self.public_crawler.save_to_database(df)
                    return len(df)
                return 0
            
            records_count = self.retry_with_backoff(registration_task)
            
            if records_count > 0:
                duration = (datetime.now() - start_time).total_seconds()
                logger.info(f"âœ… ë“±ë¡ í˜„í™© ì—…ë°ì´íŠ¸ ì™„ë£Œ ({records_count}ê±´, ì†Œìš”ì‹œê°„: {duration:.1f}ì´ˆ)")
            else:
                logger.warning("ë“±ë¡ í˜„í™© ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            logger.error(f"âŒ ë“±ë¡ í˜„í™© ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def enhanced_health_check(self):
        """í–¥ìƒëœ ì‹œìŠ¤í…œ ìƒíƒœ ì²´í¬"""
        logger.info("ğŸ¥ ì‹œìŠ¤í…œ ì¢…í•© ìƒíƒœ ì²´í¬...")
        
        health_status = {
            'database': False,
            'disk_space': False,
            'memory': False,
            'recent_errors': 0,
            'data_freshness': False
        }
        
        try:
            from database.db_helper import db_helper
            
            # 1. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
            with db_helper.get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT 1")
                cursor.fetchone()
                health_status['database'] = True
                logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ìƒ")
            
            # 2. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸
            resources = self.check_system_resources()
            if resources:
                health_status['disk_space'] = resources['disk_percent'] < 85
                health_status['memory'] = resources['memory_percent'] < 80
            
            # 3. ìµœê·¼ ì˜¤ë¥˜ í™•ì¸
            error_logs = db_helper.execute_query("""
                SELECT COUNT(*) as error_count
                FROM CrawlingLog
                WHERE status = 'ì‹¤íŒ¨' AND started_at >= DATE_SUB(NOW(), INTERVAL 24 HOUR)
            """)
            
            if error_logs:
                health_status['recent_errors'] = error_logs[0]['error_count']
                if health_status['recent_errors'] > 0:
                    logger.warning(f"âš ï¸ ìµœê·¼ 24ì‹œê°„ ì˜¤ë¥˜: {health_status['recent_errors']}ê±´")
            
            # 4. ë°ì´í„° ì‹ ì„ ë„ í™•ì¸
            latest_data = db_helper.execute_query("""
                SELECT MAX(collected_date) as latest_date
                FROM UsedCarPrice
            """)
            
            if latest_data and latest_data[0]['latest_date']:
                latest_date = latest_data[0]['latest_date']
                days_old = (datetime.now().date() - latest_date).days
                health_status['data_freshness'] = days_old <= 3
                
                if days_old > 7:
                    logger.warning(f"âš ï¸ ê°€ê²© ë°ì´í„°ê°€ {days_old}ì¼ ì „ ê²ƒì…ë‹ˆë‹¤.")
            
            # 5. ì „ì²´ ìƒíƒœ ì ìˆ˜ ê³„ì‚°
            total_checks = len([k for k in health_status.keys() if k != 'recent_errors'])
            passed_checks = sum([v for k, v in health_status.items() if k != 'recent_errors'])
            health_score = (passed_checks / total_checks) * 100
            
            logger.info(f"ğŸ¥ ì‹œìŠ¤í…œ ê±´ê°•ë„: {health_score:.0f}% ({passed_checks}/{total_checks})")
            
            # 6. í†µê³„ ìš”ì•½
            logger.info(f"ğŸ“Š ìŠ¤ì¼€ì¤„ëŸ¬ í†µê³„:")
            logger.info(f"   ì´ ì‹¤í–‰: {self.stats['total_runs']}íšŒ")
            logger.info(f"   ì„±ê³µ: {self.stats['successful_runs']}íšŒ")
            logger.info(f"   ì‹¤íŒ¨: {self.stats['failed_runs']}íšŒ")
            if self.stats['last_run']:
                logger.info(f"   ë§ˆì§€ë§‰ ì‹¤í–‰: {self.stats['last_run']}")
            
            # 7. ê±´ê°•ë„ê°€ ë‚®ìœ¼ë©´ ì•Œë¦¼
            if health_score < 70:
                self.send_email_notification(
                    f"ì‹œìŠ¤í…œ ê±´ê°•ë„ ì €í•˜ ({health_score:.0f}%)",
                    f"ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n\nìƒíƒœ ì •ë³´:\n{json.dumps(health_status, indent=2, ensure_ascii=False)}"
                )
            
        except Exception as e:
            logger.error(f"âŒ ê±´ê°• ìƒíƒœ ì²´í¬ ì‹¤íŒ¨: {e}")
            health_status['database'] = False
    
    def cleanup_old_data_enhanced(self):
        """í–¥ìƒëœ ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬"""
        logger.info("ğŸ§¹ ë°ì´í„° ì •ë¦¬ ì‹œì‘...")
        
        try:
            from database.db_helper import db_helper
            
            cleanup_stats = {}
            
            # 1. 30ì¼ ì´ìƒëœ ê°€ê²© ë°ì´í„° ì‚­ì œ (ìµœì‹  1ê°œì”©ì€ ë³´ì¡´)
            query = """
            DELETE ucp1 FROM UsedCarPrice ucp1
            WHERE ucp1.collected_date < DATE_SUB(CURDATE(), INTERVAL 30 DAY)
            AND EXISTS (
                SELECT 1 FROM UsedCarPrice ucp2
                WHERE ucp2.model_id = ucp1.model_id 
                AND ucp2.year = ucp1.year 
                AND ucp2.mileage_range = ucp1.mileage_range
                AND ucp2.collected_date > ucp1.collected_date
            )
            """
            deleted = db_helper.execute_query(query, fetch=False)
            cleanup_stats['old_prices'] = deleted
            logger.info(f"âœ… ì˜¤ë˜ëœ ê°€ê²© ë°ì´í„° {deleted}ê±´ ì •ë¦¬")
            
            # 2. 90ì¼ ì´ìƒëœ í¬ë¡¤ë§ ë¡œê·¸ ì‚­ì œ
            query = """
            DELETE FROM CrawlingLog 
            WHERE started_at < DATE_SUB(NOW(), INTERVAL 90 DAY)
            """
            deleted = db_helper.execute_query(query, fetch=False)
            cleanup_stats['old_logs'] = deleted
            logger.info(f"âœ… ì˜¤ë˜ëœ ë¡œê·¸ {deleted}ê±´ ì •ë¦¬")
            
            # 3. ì¤‘ë³µëœ ë“±ë¡ í†µê³„ ì •ë¦¬
            query = """
            DELETE rs1 FROM RegistrationStats rs1
            INNER JOIN RegistrationStats rs2
            WHERE rs1.id < rs2.id 
            AND rs1.model_id = rs2.model_id 
            AND rs1.region = rs2.region 
            AND rs1.registration_date = rs2.registration_date
            """
            deleted = db_helper.execute_query(query, fetch=False)
            cleanup_stats['duplicate_stats'] = deleted
            logger.info(f"âœ… ì¤‘ë³µ ë“±ë¡ í†µê³„ {deleted}ê±´ ì •ë¦¬")
            
            # 4. ë¹ˆ ë ˆì½”ë“œ ì •ë¦¬
            empty_tables = [
                "UsedCarPrice WHERE avg_price = 0 OR avg_price IS NULL",
                "RegistrationStats WHERE registration_count = 0 OR registration_count IS NULL"
            ]
            
            for table_condition in empty_tables:
                query = f"DELETE FROM {table_condition}"
                deleted = db_helper.execute_query(query, fetch=False)
                table_name = table_condition.split()[0]
                cleanup_stats[f'empty_{table_name}'] = deleted
                logger.info(f"âœ… {table_name} ë¹ˆ ë ˆì½”ë“œ {deleted}ê±´ ì •ë¦¬")
            
            # ì •ë¦¬ í†µê³„ ì¶œë ¥
            total_deleted = sum(cleanup_stats.values())
            logger.info(f"ğŸ‰ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ! ì´ {total_deleted}ê±´ ì •ë¦¬ë¨")
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def generate_daily_report(self):
        """ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            from database.db_helper import db_helper
            
            logger.info("ğŸ“„ ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
            
            # ì˜¤ëŠ˜ì˜ í†µê³„
            today = datetime.now().date()
            
            report = {
                'date': today.isoformat(),
                'crawling_summary': {},
                'data_summary': {},
                'system_performance': {}
            }
            
            # í¬ë¡¤ë§ ìš”ì•½
            crawling_logs = db_helper.execute_query("""
                SELECT source, status, COUNT(*) as count, SUM(records_collected) as total_records
                FROM CrawlingLog
                WHERE DATE(started_at) = %s
                GROUP BY source, status
            """, [today])
            
            for log in crawling_logs:
                source = log['source']
                if source not in report['crawling_summary']:
                    report['crawling_summary'][source] = {}
                report['crawling_summary'][source][log['status']] = {
                    'count': log['count'],
                    'records': log['total_records'] or 0
                }
            
            # ë°ì´í„° ìš”ì•½
            data_stats = db_helper.execute_query("""
                SELECT 
                    (SELECT COUNT(*) FROM CarModel) as total_models,
                    (SELECT COUNT(*) FROM UsedCarPrice WHERE collected_date = %s) as new_prices,
                    (SELECT COUNT(*) FROM RecallInfo WHERE recall_date = %s) as new_recalls,
                    (SELECT COUNT(*) FROM RegistrationStats WHERE registration_date = %s) as new_registrations
            """, [today, today, today])
            
            if data_stats:
                report['data_summary'] = data_stats[0]
            
            # ì‹œìŠ¤í…œ ì„±ëŠ¥
            if self.stats['performance_metrics']:
                recent_metrics = [m for m in self.stats['performance_metrics'] 
                                if datetime.fromisoformat(m['timestamp']).date() == today]
                
                if recent_metrics:
                    avg_duration = sum(m['duration'] for m in recent_metrics) / len(recent_metrics)
                    total_records = sum(m['records'] for m in recent_metrics)
                    
                    report['system_performance'] = {
                        'tasks_completed': len(recent_metrics),
                        'average_duration': round(avg_duration, 2),
                        'total_records_processed': total_records
                    }
            
            # ë¦¬í¬íŠ¸ ì €ì¥
            report_file = f"logs/daily_report_{today.strftime('%Y%m%d')}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"ğŸ“„ ì¼ì¼ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
            
            # ìš”ì•½ ë¡œê·¸
            logger.info("ğŸ“Š ì˜¤ëŠ˜ì˜ ìš”ì•½:")
            for source, data in report['crawling_summary'].items():
                for status, info in data.items():
                    logger.info(f"   {source} ({status}): {info['count']}íšŒ, {info['records']}ê±´")
            
        except Exception as e:
            logger.error(f"ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def setup_schedule(self):
        """ìŠ¤ì¼€ì¤„ ì„¤ì • (ê°œì„ ëœ ë²„ì „)"""
        # ë§¤ì¼ ì‹¤í–‰
        schedule.every().day.at("03:00").do(self.daily_price_update)
        schedule.every().day.at("23:30").do(self.generate_daily_report)
        
        # ë§¤ì£¼ ì‹¤í–‰
        schedule.every().monday.at("04:00").do(self.weekly_recall_update)
        schedule.every().sunday.at("02:00").do(self.cleanup_old_data_enhanced)
        schedule.every().sunday.at("01:00").do(self.backup_database)
        
        # ë§¤ì›” ì‹¤í–‰ (1ì¼)
        schedule.every().day.at("05:00").do(self._check_monthly_task)
        
        # ì‹œê°„ë³„ ì‹¤í–‰
        schedule.every().hour.do(self.enhanced_health_check)
        
        logger.info("âœ… í–¥ìƒëœ ìŠ¤ì¼€ì¤„ ì„¤ì • ì™„ë£Œ")
        logger.info("ğŸ“… ìŠ¤ì¼€ì¤„ ëª©ë¡:")
        logger.info("  - ë§¤ì¼ 03:00: ê°€ê²© ì •ë³´ ì—…ë°ì´íŠ¸")
        logger.info("  - ë§¤ì¼ 23:30: ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„±")
        logger.info("  - ë§¤ì£¼ ì›”ìš”ì¼ 04:00: ë¦¬ì½œ ì •ë³´ ì—…ë°ì´íŠ¸")
        logger.info("  - ë§¤ì£¼ ì¼ìš”ì¼ 01:00: ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…")
        logger.info("  - ë§¤ì£¼ ì¼ìš”ì¼ 02:00: ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬")
        logger.info("  - ë§¤ì›” 1ì¼ 05:00: ë“±ë¡ í˜„í™© ì—…ë°ì´íŠ¸")
        logger.info("  - ë§¤ì‹œê°„: ì‹œìŠ¤í…œ ì¢…í•© ê±´ê°• ì²´í¬")
    
    def _check_monthly_task(self):
        """ì›”ê°„ ì‘ì—… ì²´í¬ (ë§¤ì›” 1ì¼ì—ë§Œ ì‹¤í–‰)"""
        if datetime.now().day == 1:
            self.monthly_registration_update()
    
    def get_status(self):
        """í˜„ì¬ ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ë°˜í™˜"""
        return {
            'stats': self.stats.copy(),
            'next_runs': {
                job.tags[0] if job.tags else 'unknown': job.next_run
                for job in schedule.jobs
            },
            'system_status': 'running'
        }
    
    def run(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰ (ê°œì„ ëœ ë²„ì „)"""
        logger.info("ğŸš€ í–¥ìƒëœ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘...")
        
        # ì´ˆê¸° ìƒíƒœ ì²´í¬
        self.enhanced_health_check()
        
        # ìŠ¤ì¼€ì¤„ ì„¤ì •
        self.setup_schedule()
        
        # ìŠ¤ì¼€ì¤„ ì‹¤í–‰
        logger.info("â° ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. Ctrl+Cë¡œ ì¢…ë£Œí•˜ì„¸ìš”.")
        
        try:
            while True:
                schedule.run_pending()
                time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬
        except KeyboardInterrupt:
            logger.info("ğŸ›‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ...")
            
            # ì¢…ë£Œ ì „ ì •ë¦¬
            if self.encar_crawler:
                self.encar_crawler.close_driver()
            
            # ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
            logger.info("ğŸ“Š ìµœì¢… í†µê³„:")
            logger.info(f"   ì´ ì‹¤í–‰: {self.stats['total_runs']}íšŒ")
            logger.info(f"   ì„±ê³µ: {self.stats['successful_runs']}íšŒ")
            logger.info(f"   ì‹¤íŒ¨: {self.stats['failed_runs']}íšŒ")
            
            logger.info("ğŸ‘‹ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì •ìƒì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# ì‹¤í–‰
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='í–¥ìƒëœ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬')
    parser.add_argument('--test', action='store_true', help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì¦‰ì‹œ ì‹¤í–‰)')
    parser.add_argument('--task', choices=['price', 'recall', 'registration', 'health', 'cleanup', 'report', 'backup'],
                       help='íŠ¹ì • ì‘ì—…ë§Œ ì‹¤í–‰')
    parser.add_argument('--config', help='ì„¤ì • íŒŒì¼ ê²½ë¡œ (JSON)')
    
    args = parser.parse_args()
    
    scheduler = EnhancedDataScheduler()
    
    # ì„¤ì • íŒŒì¼ ë¡œë“œ
    if args.config and os.path.exists(args.config):
        try:
            with open(args.config, 'r', encoding='utf-8') as f:
                config = json.load(f)
                if 'email' in config:
                    scheduler.email_config.update(config['email'])
                if 'max_retries' in config:
                    scheduler.max_retries = config['max_retries']
                logger.info(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ: {args.config}")
        except Exception as e:
            logger.error(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    if args.test:
        logger.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
        scheduler.enhanced_health_check()
        
    elif args.task:
        logger.info(f"ğŸ¯ ë‹¨ì¼ ì‘ì—… ì‹¤í–‰: {args.task}")
        
        if args.task == 'price':
            scheduler.daily_price_update()
        elif args.task == 'recall':
            scheduler.weekly_recall_update()
        elif args.task == 'registration':
            scheduler.monthly_registration_update()
        elif args.task == 'health':
            scheduler.enhanced_health_check()
        elif args.task == 'cleanup':
            scheduler.cleanup_old_data_enhanced()
        elif args.task == 'report':
            scheduler.generate_daily_report()
        elif args.task == 'backup':
            scheduler.backup_database()
            
    else:
        scheduler.run()
