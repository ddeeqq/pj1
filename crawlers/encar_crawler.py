"""
ì—”ì¹´(encar.com) ì¤‘ê³ ì°¨ ê°€ê²© ì •ë³´ í¬ë¡¤ëŸ¬ (ê³ ë„í™” ë²„ì „)
- ê°œë³„ ì°¨ëŸ‰ ìƒì„¸ í˜ì´ì§€ì—ì„œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
- ì˜µì…˜, ì‚¬ê³ ì´ë ¥, ì†Œìœ ì ë³€ê²½ì´ë ¥, ì°¨ëŒ€ë²ˆí˜¸ ë“± ìˆ˜ì§‘
"""
import re
import time
import pandas as pd
from datetime import datetime
import sys
import os
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from urllib.parse import urljoin

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from database.db_helper import db_helper
from config.logging_config import get_crawler_logger

logger = get_crawler_logger('encar_detailed')

class EncarCrawler:
    def __init__(self, config):
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì‹œ ì„¤ì •(config)ì„ ì „ë‹¬ë°›ìŒ"""
        self.config = config
        self.driver = None
        self.base_url = "http://www.encar.com" # USER ACTION: ì—”ì¹´ ì‚¬ì´íŠ¸ì˜ ê¸°ë³¸ URLì„ í™•ì¸í•´ì£¼ì„¸ìš”.

    def setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
        if self.driver:
            return
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument(f"--user-agent={self.config.get('user_agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')}")
        
        try:
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            logger.info("âœ… Chrome ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ Chrome ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
            
    def close_driver(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            self.driver = None
            logger.info("ë“œë¼ì´ë²„ ì¢…ë£Œ ì™„ë£Œ")
            
    def get_car_detail_urls(self, manufacturer, model_name, year=None):
        """ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì—ì„œ ê°œë³„ ì°¨ëŸ‰ì˜ ìƒì„¸ í˜ì´ì§€ URL ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        detail_urls = []
        try:
            if not self.driver:
                self.setup_driver()
                
            search_query = f"{manufacturer} {model_name}"
            if year:
                search_query += f" {year}"
            
            # USER ACTION: ì—”ì¹´ì˜ ì‹¤ì œ ê²€ìƒ‰ URL êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •í•´ì£¼ì„¸ìš”.
            search_url_template = self.config.get('search_url', 'http://www.encar.com/dc/dc_carsearchlist.do?carType=kor&searchType=model&searchKey=&prcStart=&prcEnd=&mileStart=&mileEnd=&yearMin=&yearMax=&trans=&fuel=&disp=&size=&color=&options=&q={query}')
            url = search_url_template.format(query=search_query)
            
            logger.info(f"ğŸ” ìƒì„¸ í˜ì´ì§€ URL ìˆ˜ì§‘ ì¤‘: {search_query}")
            self.driver.get(url)
            time.sleep(self.config.get('delay', 2))
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # USER ACTION: ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡ì—ì„œ ê° ì°¨ëŸ‰ì˜ ìƒì„¸ í˜ì´ì§€ ë§í¬ë¥¼ ê°€ë¦¬í‚¤ëŠ” CSS ì„ íƒìë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.
            link_selector = "a.car-item-link, div.list-item > a"
            links = soup.select(link_selector)
            
            max_items = self.config.get('max_items_per_model', 10) # ë„ˆë¬´ ë§ì€ ì°¨ëŸ‰ì„ ìˆ˜ì§‘í•˜ì§€ ì•Šë„ë¡ ì œí•œ
            
            for link in links[:max_items]:
                href = link.get('href')
                if href:
                    full_url = urljoin(self.base_url, href)
                    if full_url not in detail_urls:
                        detail_urls.append(full_url)
            
            logger.info(f"âœ… {len(detail_urls)}ê°œì˜ ìƒì„¸ í˜ì´ì§€ URL ìˆ˜ì§‘ ì™„ë£Œ")
            return detail_urls
            
        except Exception as e:
            logger.error(f"ìƒì„¸ í˜ì´ì§€ URL ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            return []

    def get_car_detail(self, detail_url):
        """ì°¨ëŸ‰ ìƒì„¸ í˜ì´ì§€ë¥¼ ë°©ë¬¸í•˜ì—¬ ëª¨ë“  ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            self.driver.get(detail_url)
            time.sleep(self.config.get('delay', 1)) # í˜ì´ì§€ ë¡œë“œë¥¼ ìœ„í•œ ëŒ€ê¸°
            
            # USER ACTION: "ì„±ëŠ¥Â·ìƒíƒœ ì ê²€ê¸°ë¡ë¶€" íŒì—…ì´ë‚˜ í”„ë ˆì„ì´ ìˆë‹¤ë©´, í•´ë‹¹ ìš”ì†Œë¥¼ í´ë¦­í•˜ê±°ë‚˜ ì „í™˜í•˜ëŠ” ì½”ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.
            # ì˜ˆ: WebDriverWait(self.driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.ID, "performance_check_frame")))
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            car_info = {
                'source_url': detail_url,
                'collected_date': datetime.now().date()
            }

            # --- ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ ---
            car_info['price'] = self._extract_price(soup)
            car_info['year'] = self._extract_year(soup)
            car_info['mileage'] = self._extract_mileage(soup)
            car_info['manufacturer'], car_info['model_name'], car_info['trim_name'] = self._extract_model_and_trim(soup)
            
            # --- ìƒì„¸ ì •ë³´ ì¶”ì¶œ ---
            car_info['vin'] = self._extract_vin(soup) # ì°¨ëŒ€ë²ˆí˜¸
            car_info['options'] = self._extract_options(soup) # ì˜µì…˜
            car_info['ownership_history'] = self._extract_ownership_history(soup) # ì†Œìœ ì ë³€ê²½ ì´ë ¥
            
            # --- ì„±ëŠ¥ ì ê²€ ê¸°ë¡ë¶€ ì •ë³´ ì¶”ì¶œ ---
            performance_data = self._extract_performance_check(soup)
            car_info.update(performance_data)

            return car_info

        except Exception as e:
            logger.error(f"ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜ ({detail_url}): {e}")
            return None

    # --- ë°ì´í„° ì¶”ì¶œ í—¬í¼ í•¨ìˆ˜ë“¤ ---

    def _extract_price(self, soup):
        # USER ACTION: ê°€ê²© ì •ë³´ê°€ ìˆëŠ” ìš”ì†Œì˜ CSS ì„ íƒìë¥¼ í™•ì¸í•˜ê³  ìˆ˜ì •í•´ì£¼ì„¸ìš”.
        price_selector = "span.price, strong.cost"
        elem = soup.select_one(price_selector)
        if elem:
            numbers = re.findall(r'[\d,]+', elem.text)
            if numbers:
                return int(numbers[0].replace(',', '')) * 10000 # 'ë§Œì›' ë‹¨ìœ„ ì²˜ë¦¬
        return 0

    def _extract_year(self, soup):
        # USER ACTION: ì—°ì‹ ì •ë³´ê°€ ìˆëŠ” ìš”ì†Œì˜ CSS ì„ íƒìë¥¼ í™•ì¸í•˜ê³  ìˆ˜ì •í•´ì£¼ì„¸ìš”.
        year_selector = "span.year, div.car-info .year"
        elem = soup.select_one(year_selector)
        if elem:
            match = re.search(r'(20\d{2})|(\d{2}ë…„)', elem.text)
            if match:
                return int(match.group(1)) if match.group(1) else 2000 + int(match.group(2)[:2])
        return None

    def _extract_mileage(self, soup):
        # USER ACTION: ì£¼í–‰ê±°ë¦¬ ì •ë³´ê°€ ìˆëŠ” ìš”ì†Œì˜ CSS ì„ íƒìë¥¼ í™•ì¸í•˜ê³  ìˆ˜ì •í•´ì£¼ì„¸ìš”.
        mileage_selector = "span.mileage, div.car-info .km"
        elem = soup.select_one(mileage_selector)
        if elem:
            numbers = re.findall(r'[\d,]+', elem.text)
            if numbers:
                mileage = int(numbers[0].replace(',', ''))
                return mileage * 10000 if 'ë§Œ' in elem.text else mileage
        return 0

    def _extract_model_and_trim(self, soup):
        # USER ACTION: ì œì¡°ì‚¬, ëª¨ë¸ëª…, ì„¸ë¶€ íŠ¸ë¦¼ëª…ì´ í¬í•¨ëœ ìš”ì†Œì˜ CSS ì„ íƒìë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.
        title_selector = "h1.car-name, div.car-title"
        elem = soup.select_one(title_selector)
        if elem:
            full_title = elem.text.strip()
            # ì˜ˆ: "í˜„ëŒ€ ê·¸ëœì € IG 2.4 í”„ë¦¬ë¯¸ì—„" -> ["í˜„ëŒ€", "ê·¸ëœì € IG", "2.4 í”„ë¦¬ë¯¸ì—„"]
            # ì‹¤ì œ íŒŒì‹± ë¡œì§ì€ ì‚¬ì´íŠ¸ì˜ ì œëª© êµ¬ì¡°ì— ë”°ë¼ ë§¤ìš° ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            parts = full_title.split()
            if len(parts) >= 3:
                return parts[0], " ".join(parts[1:-1]), parts[-1]
            elif len(parts) == 2:
                return parts[0], parts[1], None
            elif len(parts) == 1:
                return None, parts[0], None
        return None, None, None

    def _extract_vin(self, soup):
        # USER ACTION: ì°¨ëŒ€ë²ˆí˜¸ê°€ ìˆëŠ” ìš”ì†Œì˜ CSS ì„ íƒìë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”. (ì„±ëŠ¥ì ê²€ê¸°ë¡ë¶€ ë‚´ì— ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŒ)
        vin_selector = "td.vin, span#carVin"
        elem = soup.select_one(vin_selector)
        if elem:
            return elem.text.strip()
        return None

    def _extract_options(self, soup):
        # USER ACTION: ì˜µì…˜ ëª©ë¡ì„ í¬í•¨í•˜ëŠ” ê° ì˜µì…˜ í•­ëª©ì˜ CSS ì„ íƒìë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.
        option_selector = "ul.options-list > li, div.options .item"
        options = [opt.text.strip() for opt in soup.select(option_selector)]
        return options if options else []

    def _extract_ownership_history(self, soup):
        # USER ACTION: ì†Œìœ ì ë³€ê²½ ì´ë ¥ íšŸìˆ˜ ì •ë³´ê°€ ìˆëŠ” ìš”ì†Œì˜ CSS ì„ íƒìë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.
        owner_selector = "td.owner-changes"
        elem = soup.select_one(owner_selector)
        if elem:
            numbers = re.findall(r'\d+', elem.text)
            return int(numbers[0]) if numbers else 0
        return 0

    def _extract_performance_check(self, soup):
        """ì„±ëŠ¥Â·ìƒíƒœ ì ê²€ê¸°ë¡ë¶€ì—ì„œ ì‚¬ê³ ì´ë ¥, ëˆ„ìœ , íŠ¹ì´ì‚¬í•­ ë“±ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        data = {
            'accident_type': 'ì—†ìŒ', # ì—†ìŒ, ë¬´ì‚¬ê³ (ë‹¨ìˆœìˆ˜ë¦¬), ì‚¬ê³ 
            'accident_details': [],
            'leakage_points': [],
            'special_notes': ''
        }
        
        # USER ACTION: ì„±ëŠ¥ì ê²€ê¸°ë¡ë¶€ ì˜ì—­ì„ ê°€ë¦¬í‚¤ëŠ” CSS ì„ íƒìë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.
        perf_check_area = soup.select_one("div#performanceCheck, table.performance-table")
        if not perf_check_area:
            return data

        # ì‚¬ê³ ìœ ë¬´ (í”„ë ˆì„ ì‚¬ê³ )
        # USER ACTION: 'ì£¼ìš”ê³¨ê²©' ë˜ëŠ” 'ì‚¬ê³ ' ê´€ë ¨ í•­ëª©ì˜ CSS ì„ íƒìë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.
        frame_damage_selector = "td.frame-damage-check"
        frame_elem = perf_check_area.select_one(frame_damage_selector)
        if frame_elem and ('ìˆìŒ' in frame_elem.text or 'ì‚¬ê³ ' in frame_elem.text):
            data['accident_type'] = 'ì‚¬ê³ '

        # ë‹¨ìˆœìˆ˜ë¦¬ (ì™¸íŒ)
        # USER ACTION: 'ë‹¨ìˆœìˆ˜ë¦¬' ë˜ëŠ” 'ì™¸íŒ' ê´€ë ¨ í•­ëª©ì˜ CSS ì„ íƒìë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.
        panel_repair_selector = "td.panel-repair-check"
        panel_elem = perf_check_area.select_one(panel_repair_selector)
        if panel_elem and ('ìˆìŒ' in panel_elem.text or 'êµí™˜' in panel_elem.text):
            if data['accident_type'] == 'ì—†ìŒ':
                data['accident_type'] = 'ë¬´ì‚¬ê³ (ë‹¨ìˆœìˆ˜ë¦¬)'

        # ì‚¬ê³ /ìˆ˜ë¦¬ ë¶€ìœ„ ìƒì„¸
        # USER ACTION: ìˆ˜ë¦¬ëœ ê° ë¶€ìœ„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìš”ì†Œì˜ CSS ì„ íƒìë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”. (ì˜ˆ: ì²´í¬ëœ ì´ë¯¸ì§€)
        repaired_parts_selector = "td.repaired"
        repaired_parts = [part.get('data-part-name', part.text.strip()) for part in perf_check_area.select(repaired_parts_selector)]
        data['accident_details'] = repaired_parts

        # ëˆ„ìœ  ì •ë³´
        # USER ACTION: ëˆ„ìœ ê°€ ì²´í¬ëœ í•­ëª©ì˜ CSS ì„ íƒìë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.
        leakage_selector = "td.leakage-point.checked"
        leaks = [leak.get('data-part-name', leak.text.strip()) for leak in perf_check_area.select(leakage_selector)]
        data['leakage_points'] = leaks
        
        # íŠ¹ì´ì‚¬í•­
        # USER ACTION: íŠ¹ì´ì‚¬í•­ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ìš”ì†Œì˜ CSS ì„ íƒìë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.
        notes_selector = "td.special-notes"
        notes_elem = perf_check_area.select_one(notes_selector)
        if notes_elem:
            data['special_notes'] = notes_elem.text.strip()

        return data

    def crawl_and_save(self, car_list):
        """ì£¼ì–´ì§„ ì°¨ì¢… ëª©ë¡ì— ëŒ€í•´ ìƒì„¸ ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ê³  DBì— ì €ì¥í•©ë‹ˆë‹¤."""
        db_helper.update_crawling_log('encar_detailed', 'ì‹œì‘')
        total_collected = 0
        
        try:
            self.setup_driver()
            for car_spec in car_list:
                manufacturer = car_spec['manufacturer']
                model_name = car_spec['model_name']
                
                model_id = db_helper.get_or_insert_car_model(manufacturer, model_name)
                if not model_id:
                    logger.warning(f"ëª¨ë¸ IDë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {manufacturer} {model_name}")
                    continue

                logger.info(f"--- {manufacturer} {model_name} ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘ ---")
                detail_urls = self.get_car_detail_urls(manufacturer, model_name)
                
                collected_for_model = 0
                for url in detail_urls:
                    car_detail = self.get_car_detail(url)
                    if car_detail:
                        car_detail['model_id'] = model_id
                        # USER ACTION: db_helperì— ìƒì„¸ ì°¨ëŸ‰ ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ë©”ì†Œë“œë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.
                        # ì˜ˆ: db_helper.insert_detailed_used_car(car_detail)
                        logger.info(f"  - ìˆ˜ì§‘ ì„±ê³µ: {car_detail.get('trim_name', 'N/A')}, {car_detail.get('price')}ë§Œì›")
                        # ì§€ê¸ˆì€ ë‹¨ìˆœíˆ ë¡œê·¸ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤. DB ì €ì¥ ë¡œì§ì„ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.
                        collected_for_model += 1
                    time.sleep(self.config.get('delay', 2)) # ê° ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼ ì‚¬ì´ì˜ ë”œë ˆì´

                logger.info(f"--- {manufacturer} {model_name}: {collected_for_model}ê±´ ìˆ˜ì§‘ ì™„ë£Œ ---")
                total_collected += collected_for_model

            db_helper.update_crawling_log('encar_detailed', 'ì™„ë£Œ', total_collected)
            logger.info(f"ğŸ‰ ì „ì²´ ìƒì„¸ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {total_collected}ê±´ ìˆ˜ì§‘")
            
        except Exception as e:
            db_helper.update_crawling_log('encar_detailed', 'ì‹¤íŒ¨', total_collected, str(e))
            logger.error(f"ìƒì„¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        finally:
            self.close_driver()

if __name__ == '__main__':
    import json
    print("ìƒì„¸ ì •ë³´ í¬ë¡¤ëŸ¬ ë‹¨ë… í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    
    try:
        with open('config/scheduler_config.json', 'r', encoding='utf-8') as f:
            full_config = json.load(f)
        encar_config = full_config['crawling']['encar']
        print("âœ… í…ŒìŠ¤íŠ¸ ì„¤ì • ë¡œë“œ ì™„ë£Œ")
        
        crawler = EncarCrawler(config=encar_config)
        
        test_cars = [
            {'manufacturer': 'í˜„ëŒ€', 'model_name': 'ê·¸ëœì € IG'},
        ]
        
        # ì‹¤ì œ í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.
        # print("í…ŒìŠ¤íŠ¸ ìƒì„¸ í¬ë¡¤ë§:")
        # crawler.crawl_and_save(test_cars)
        print("ì‹¤ì œ ì‹¤í–‰í•˜ë ¤ë©´ `crawl_and_save` ë©”ì†Œë“œì˜ ì£¼ì„ì„ í•´ì œí•˜ê³ ,")
        print("USER ACTION ì£¼ì„ì´ ë‹¬ë¦° ë¶€ë¶„ì˜ URLê³¼ CSS ì„ íƒìë¥¼ ì‹¤ì œ ì‚¬ì´íŠ¸ì— ë§ê²Œ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        
    except FileNotFoundError:
        print("ì˜¤ë¥˜: config/scheduler_config.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")