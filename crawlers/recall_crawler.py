"""
ê°œì„ ëœ ìë™ì°¨ ë¦¬ì½œ ì„¼í„° ì •ë³´ í¬ë¡¤ëŸ¬
ì‹¤ì œ URL êµ¬ì¡°ì™€ íŒŒë¼ë¯¸í„°ë¥¼ ë°˜ì˜í•œ ë²„ì „
"""
import requests
from bs4 import BeautifulSoup
import time
import logging
import pandas as pd
from datetime import datetime, timedelta
import re
import sys
import os
from urllib.parse import urljoin, urlparse

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“ˆ importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from database.db_helper import db_helper

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RecallCrawler:
    def __init__(self, config=None):
        self.config = config or {}
        
        # ì‹¤ì œ ìë™ì°¨ë¦¬ì½œì„¼í„° URL êµ¬ì¡°
        self.base_url = "https://www.car.go.kr"
        self.recall_list_url = "https://www.car.go.kr/ri/stat/list.do"
        self.recall_detail_url = "https://www.car.go.kr/ri/stat/view.do"
        self.car_check_url = "https://www.car.go.kr/ri/recall/list.do"

        self.delay = self.config.get('delay', 2)
        self.max_retries = self.config.get('max_retries', 3)

        # ì„¸ì…˜ ì„¤ì •
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.6,en;q=0.4',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })

        # ì‹¬ê°ë„ íŒë‹¨ í‚¤ì›Œë“œ
        self.severity_keywords = {
            'ë§¤ìš°ì‹¬ê°': ['í™”ì¬', 'í­ë°œ', 'ì‚¬ë§', 'ì¤‘ìƒ', 'ì—ì–´ë°±', 'ë¸Œë ˆì´í¬', 'ì¡°í–¥ì¥ì¹˜', 'ê¸‰ê°€ì†', 'ê¸‰ì •ì§€'],
            'ì‹¬ê°': ['ì—”ì§„', 'ë³€ì†ê¸°', 'ì—°ë£Œ', 'ë°°ì¶œê°€ìŠ¤', 'ì „ê¸°ê³„í†µ', 'íƒ€ì´ì–´', 'ì„œìŠ¤íœì…˜'],
            'ë³´í†µ': ['ëˆ„ìˆ˜', 'ì†ŒìŒ', 'ì§„ë™', 'ì„¼ì„œ', 'ë¨í”„', 'ê³„ê¸°íŒ', 'ê³µì¡°ì¥ì¹˜'],
            'ê²½ë¯¸': ['ë„ìƒ‰', 'ë‚´ì¥ì¬', 'í¸ì˜ì¥ì¹˜', 'ì˜¤ë””ì˜¤', 'ë„¤ë¹„ê²Œì´ì…˜', 'USB']
        }

    def search_recall_info(self, manufacturer=None, model_name=None, start_date=None, end_date=None, **kwargs):
        """ë¦¬ì½œ ì •ë³´ ê²€ìƒ‰"""
        recall_data = []

        try:
            # ê²€ìƒ‰ íŒŒë¼ë¯¸í„° êµ¬ì„±
            params = self._build_search_params(manufacturer, model_name, start_date, end_date, **kwargs)

            # ì²« í˜ì´ì§€ ìš”ì²­
            response = self._make_request(self.recall_list_url, params)
            if not response:
                return recall_data

            soup = BeautifulSoup(response.text, 'html.parser')

            # ì „ì²´ í˜ì´ì§€ ìˆ˜ í™•ì¸
            total_pages = self._get_total_pages(soup)
            max_pages = kwargs.get('max_pages', min(total_pages, 10))  # ìµœëŒ€ 10í˜ì´ì§€

            logger.info(f"ğŸ” ì´ {total_pages}í˜ì´ì§€ ì¤‘ {max_pages}í˜ì´ì§€ê¹Œì§€ í¬ë¡¤ë§")

            # ê° í˜ì´ì§€ë³„ í¬ë¡¤ë§
            for page in range(1, max_pages + 1):
                logger.info(f"ğŸ“„ {page}/{max_pages} í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")

                if page > 1:
                    params['pageIndex'] = page
                    response = self._make_request(self.recall_list_url, params)
                    if not response:
                        continue
                    soup = BeautifulSoup(response.text, 'html.parser')

                # í˜ì´ì§€ë³„ ë¦¬ì½œ ì •ë³´ ì¶”ì¶œ
                page_data = self._parse_recall_list(soup, manufacturer, model_name)
                recall_data.extend(page_data)

                time.sleep(self.delay)

            logger.info(f"âœ… ì´ {len(recall_data)}ê±´ì˜ ë¦¬ì½œ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"ë¦¬ì½œ ì •ë³´ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")

        return recall_data

    def _build_search_params(self, manufacturer, model_name, start_date, end_date, **kwargs):
        """ê²€ìƒ‰ íŒŒë¼ë¯¸í„° êµ¬ì„±"""
        params = {
            'pageIndex': 1,
            'pageUnit': kwargs.get('page_size', 20),
            'searchCondition': '',
            'searchKeyword': ''
        }

        # ì œì¡°ì‚¬ ê²€ìƒ‰
        if manufacturer:
            if params['searchKeyword']:
                params['searchKeyword'] += f" {manufacturer}"
            else:
                params['searchKeyword'] = manufacturer

        # ëª¨ë¸ëª… ê²€ìƒ‰
        if model_name:
            if params['searchKeyword']:
                params['searchKeyword'] += f" {model_name}"
            else:
                params['searchKeyword'] = model_name

        # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        if start_date:
            params['searchStartDate'] = start_date
        if end_date:
            params['searchEndDate'] = end_date

        # ì¶”ê°€ í•„í„°
        if kwargs.get('recall_type'):
            params['recallType'] = kwargs['recall_type']

        return params

    def _make_request(self, url, params=None, retries=0):
        """HTTP ìš”ì²­ ì‹¤í–‰ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        try:
            response = self.session.get(url, params=params, timeout=30)
            response.raise_for_status()
            return response

        except requests.exceptions.RequestException as e:
            if retries < self.max_retries:
                logger.warning(f"ìš”ì²­ ì‹¤íŒ¨, ì¬ì‹œë„ {retries + 1}/{self.max_retries}: {e}")
                time.sleep(self.delay * (retries + 1))
                return self._make_request(url, params, retries + 1)
            else:
                logger.error(f"ìš”ì²­ ìµœì¢… ì‹¤íŒ¨: {e}")
                return None

    def _get_total_pages(self, soup):
        """ì „ì²´ í˜ì´ì§€ ìˆ˜ í™•ì¸"""
        try:
            # í˜ì´ì§• ì •ë³´ì—ì„œ ë§ˆì§€ë§‰ í˜ì´ì§€ ì¶”ì¶œ
            paging = soup.select_one('.paging, .pagination, .page_wrap')
            if paging:
                page_links = paging.select('a')
                if page_links:
                    # ìˆ«ìë¡œ ëœ í˜ì´ì§€ ë§í¬ ì¤‘ ê°€ì¥ í° ê°’ ì°¾ê¸°
                    max_page = 1
                    for link in page_links:
                        text = link.get_text(strip=True)
                        if text.isdigit():
                            max_page = max(max_page, int(text))
                    return max_page

            return 1
        except Exception as e:
            logger.debug(f"í˜ì´ì§€ ìˆ˜ í™•ì¸ ì˜¤ë¥˜: {e}")
            return 1

    def _parse_recall_list(self, soup, manufacturer, model_name):
        """ë¦¬ì½œ ëª©ë¡ í˜ì´ì§€ íŒŒì‹±"""
        recall_data = []

        try:
            # ë¦¬ì½œ ëª©ë¡ í…Œì´ë¸” ì°¾ê¸°
            table = soup.select_one('.board_list table, .list_table, tbody')
            if not table:
                logger.warning("ë¦¬ì½œ ëª©ë¡ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return recall_data

            # ê° í–‰(ë¦¬ì½œ í•­ëª©) íŒŒì‹±
            rows = table.select('tr')[1:]  # í—¤ë” ì œì™¸

            for row in rows:
                try:
                    recall_info = self._extract_recall_info(row, manufacturer, model_name)
                    if recall_info:
                        # ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                        detail_info = self._get_recall_detail(recall_info.get('detail_url'))
                        if detail_info:
                            recall_info.update(detail_info)

                        recall_data.append(recall_info)

                except Exception as e:
                    logger.debug(f"ê°œë³„ ë¦¬ì½œ í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

        except Exception as e:
            logger.error(f"ë¦¬ì½œ ëª©ë¡ íŒŒì‹± ì˜¤ë¥˜: {e}")

        return recall_data

    def _extract_recall_info(self, row, manufacturer, model_name):
        """ê°œë³„ ë¦¬ì½œ ì •ë³´ ì¶”ì¶œ"""
        try:
            cells = row.select('td')
            if len(cells) < 6:
                return None

            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            recall_info = {
                'manufacturer': manufacturer or self._clean_text(cells[0].get_text()),
                'model_name': model_name or self._clean_text(cells[1].get_text()),
                'recall_date': self._parse_date(cells[2].get_text()),
                'recall_title': self._clean_text(cells[3].get_text()),
                'recall_reason': self._clean_text(cells[4].get_text()),
                'affected_units': self._extract_number(cells[5].get_text()),
                'source': 'car.go.kr',
                'collected_date': datetime.now().date()
            }

            # ìƒì„¸ í˜ì´ì§€ URL ì¶”ì¶œ
            detail_link = row.select_one('a[href]')
            if detail_link:
                href = detail_link.get('href')
                recall_info['detail_url'] = urljoin(self.base_url, href)

            # ì‹¬ê°ë„ ìë™ íŒë‹¨
            recall_info['severity_level'] = self._determine_severity(
                recall_info['recall_title'], 
                recall_info['recall_reason']
            )

            return recall_info

        except Exception as e:
            logger.debug(f"ë¦¬ì½œ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None

    def _get_recall_detail(self, detail_url):
        """ë¦¬ì½œ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        if not detail_url:
            return {}

        try:
            response = self._make_request(detail_url)
            if not response:
                return {}

            soup = BeautifulSoup(response.text, 'html.parser')

            detail_info = {}

            # ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            content_area = soup.select_one('.view_content, .content_area, .detail_content')
            if content_area:
                # ê²°í•¨ ë‚´ìš©
                defect_elem = content_area.select_one('.defect_content, .fault_detail')
                if defect_elem:
                    detail_info['defect_content'] = self._clean_text(defect_elem.get_text())

                # ì‹œì • ë°©ë²•
                correction_elem = content_area.select_one('.correction_method, .repair_method')
                if correction_elem:
                    detail_info['correction_method'] = self._clean_text(correction_elem.get_text())

                # ìƒì‚° ê¸°ê°„
                production_elem = content_area.select_one('.production_period, .manufacture_date')
                if production_elem:
                    detail_info['production_period'] = self._clean_text(production_elem.get_text())

            time.sleep(0.5)  # ìƒì„¸ í˜ì´ì§€ ìš”ì²­ ê°„ê²©
            return detail_info

        except Exception as e:
            logger.debug(f"ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return {}

    def _clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        if not text:
            return ""
        return re.sub(r'\s+', ' ', text.strip())

    def _parse_date(self, date_str):
        """ë‚ ì§œ íŒŒì‹±"""
        try:
            date_str = self._clean_text(date_str)

            # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì‹œë„
            date_formats = [
                '%Y-%m-%d',
                '%Y.%m.%d', 
                '%Y/%m/%d',
                '%Yë…„ %mì›” %dì¼',
                '%Y.%m.%d.'
            ]

            for fmt in date_formats:
                try:
                    return datetime.strptime(date_str, fmt).date()
                except ValueError:
                    continue

            # ë…„ì›”ë§Œ ìˆëŠ” ê²½ìš°
            year_month_match = re.search(r'(\d{4})[.-]?(\d{1,2})', date_str)
            if year_month_match:
                year, month = year_month_match.groups()
                return datetime(int(year), int(month), 1).date()

            return None

        except Exception as e:
            logger.debug(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None

    def _extract_number(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ"""
        try:
            if not text:
                return 0

            # ì‰¼í‘œ ì œê±° í›„ ìˆ«ì ì¶”ì¶œ
            numbers = re.findall(r'[\d,]+', text.replace(',', ''))
            if numbers:
                # ê°€ì¥ í° ìˆ«ìë¥¼ ì„ íƒ (ëŒ€ìƒ ìˆ˜ëŸ‰ìœ¼ë¡œ ì¶”ì •)
                return max([int(num.replace(',', '')) for num in numbers if num.replace(',', '').isdigit()])

            return 0

        except Exception as e:
            logger.debug(f"ìˆ«ì ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return 0

    def _determine_severity(self, title, reason):
        """ì‹¬ê°ë„ ìë™ íŒë‹¨"""
        try:
            text = (title + ' ' + reason).lower()

            for severity, keywords in self.severity_keywords.items():
                if any(keyword in text for keyword in keywords):
                    return severity

            return 'ê²½ë¯¸'

        except Exception as e:
            logger.debug(f"ì‹¬ê°ë„ íŒë‹¨ ì˜¤ë¥˜: {e}")
            return 'ì•Œìˆ˜ì—†ìŒ'

    def search_recall_by_car_number(self, car_number):
        """ì°¨ëŸ‰ë²ˆí˜¸ë¡œ ë¦¬ì½œ ëŒ€ìƒ í™•ì¸"""
        try:
            params = {
                'carNumber': car_number
            }

            response = self._make_request(self.car_check_url, params)
            if not response:
                return []

            soup = BeautifulSoup(response.text, 'html.parser')

            # ë¦¬ì½œ ëŒ€ìƒ ì—¬ë¶€ í™•ì¸ ë° ì •ë³´ ì¶”ì¶œ
            recall_results = []
            result_area = soup.select_one('.result_area, .recall_result')

            if result_area:
                recall_items = result_area.select('.recall_item, .result_item')
                for item in recall_items:
                    recall_info = self._extract_car_recall_info(item, car_number)
                    if recall_info:
                        recall_results.append(recall_info)

            return recall_results

        except Exception as e:
            logger.error(f"ì°¨ëŸ‰ë²ˆí˜¸ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []

    def _extract_car_recall_info(self, item, car_number):
        """ì°¨ëŸ‰ë³„ ë¦¬ì½œ ì •ë³´ ì¶”ì¶œ"""
        try:
            return {
                'car_number': car_number,
                'manufacturer': self._clean_text(item.select_one('.manufacturer, .company').get_text() if item.select_one('.manufacturer, .company') else ''),
                'model_name': self._clean_text(item.select_one('.model, .car_name').get_text() if item.select_one('.model, .car_name') else ''),
                'recall_reason': self._clean_text(item.select_one('.reason, .cause').get_text() if item.select_one('.reason, .cause') else ''),
                'recall_status': self._clean_text(item.select_one('.status, .progress').get_text() if item.select_one('.status, .progress') else ''),
                'correction_period': self._clean_text(item.select_one('.period, .date').get_text() if item.select_one('.period, .date') else ''),
                'source': 'car.go.kr',
                'collected_date': datetime.now().date()
            }

        except Exception as e:
            logger.debug(f"ì°¨ëŸ‰ ë¦¬ì½œ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None

    def crawl_and_save(self, car_list=None, date_range_days=30):
        """ë¦¬ì½œ ì •ë³´ í¬ë¡¤ë§ ë° DB ì €ì¥"""
        db_helper.update_crawling_log('recall', 'ì‹œì‘')
        total_collected = 0

        try:
            # ë‚ ì§œ ë²”ìœ„ ì„¤ì • (ìµœê·¼ 30ì¼)
            end_date = datetime.now().date()
            start_date = end_date - timedelta(days=date_range_days)

            if car_list:
                # íŠ¹ì • ì°¨ëŸ‰ ëª¨ë¸ë“¤ì˜ ë¦¬ì½œ ì •ë³´ ìˆ˜ì§‘
                for car in car_list:
                    manufacturer = car['manufacturer']
                    model_name = car['model_name']

                    logger.info(f"ğŸš— {manufacturer} {model_name} ë¦¬ì½œ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")

                    # ëª¨ë¸ ID ì¡°íšŒ ë˜ëŠ” ìƒì„±
                    model_id = db_helper.get_car_model_id(manufacturer, model_name)
                    if not model_id:
                        db_helper.insert_car_model(manufacturer, model_name)
                        model_id = db_helper.get_car_model_id(manufacturer, model_name)

                    # ë¦¬ì½œ ì •ë³´ ê²€ìƒ‰
                    recall_data = self.search_recall_info(
                        manufacturer=manufacturer,
                        model_name=model_name,
                        start_date=start_date.strftime('%Y-%m-%d'),
                        end_date=end_date.strftime('%Y-%m-%d'),
                        max_pages=5
                    )

                    # DBì— ì €ì¥
                    for recall in recall_data:
                        db_helper.insert_recall_info(
                            model_id=model_id,
                            **recall
                        )

                    total_collected += len(recall_data)
                    time.sleep(self.delay)

            else:
                # ì „ì²´ ë¦¬ì½œ ì •ë³´ ìˆ˜ì§‘ (ìµœê·¼ ë°œìƒí•œ ë¦¬ì½œë“¤)
                logger.info(f"ğŸ” ì „ì²´ ë¦¬ì½œ ì •ë³´ ìˆ˜ì§‘ ì¤‘ ({start_date} ~ {end_date})")

                recall_data = self.search_recall_info(
                    start_date=start_date.strftime('%Y-%m-%d'),
                    end_date=end_date.strftime('%Y-%m-%d'),
                    max_pages=10
                )

                # ì œì¡°ì‚¬/ëª¨ë¸ë³„ë¡œ ê·¸ë£¹í•‘í•˜ì—¬ ì €ì¥
                for recall in recall_data:
                    manufacturer = recall['manufacturer']
                    model_name = recall['model_name']

                    # ëª¨ë¸ ID ì¡°íšŒ ë˜ëŠ” ìƒì„±
                    model_id = db_helper.get_car_model_id(manufacturer, model_name)
                    if not model_id:
                        db_helper.insert_car_model(manufacturer, model_name)
                        model_id = db_helper.get_car_model_id(manufacturer, model_name)

                    # DBì— ì €ì¥
                    recall['model_id'] = model_id
                    db_helper.insert_recall_info(**recall)

                total_collected = len(recall_data)

            db_helper.update_crawling_log('recall', 'ì™„ë£Œ', total_collected)
            logger.info(f"ğŸ‰ ë¦¬ì½œ ì •ë³´ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {total_collected}ê±´ ìˆ˜ì§‘")

        except Exception as e:
            db_helper.update_crawling_log('recall', 'ì‹¤íŒ¨', total_collected, str(e))
            logger.error(f"ë¦¬ì½œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

        finally:
            self.session.close()


if __name__ == '__main__':
    print("RecallCrawler ëª¨ë“ˆ")
    print("ì´ ëª¨ë“ˆì€ run.pyë¥¼ í†µí•´ ì‹¤í–‰í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ importí•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.")
    print("ì˜ˆì‹œ: python run.py crawl")