"""
ìë™ì°¨ ë¦¬ì½œ ì„¼í„° ì •ë³´ í¬ë¡¤ëŸ¬
"""
import requests
from bs4 import BeautifulSoup
import time
import logging
import pandas as pd
from datetime import datetime
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.config import CRAWLING_CONFIG
from database.db_helper import db_helper

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RecallCrawler:
    def __init__(self):
        self.config = CRAWLING_CONFIG['recall']
        self.base_url = self.config['base_url']
        self.search_url = self.config['search_url']
        self.delay = self.config['delay']
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
    def search_recall_info(self, manufacturer, model_name):
        """íŠ¹ì • ì°¨ëŸ‰ì˜ ë¦¬ì½œ ì •ë³´ ê²€ìƒ‰"""
        recall_data = []
        
        try:
            # ë¦¬ì½œì„¼í„° API í˜¸ì¶œ (ì‹¤ì œ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
            params = {
                'manufacturer': manufacturer,
                'model': model_name,
                'pageSize': 100
            }
            
            response = self.session.get(self.search_url, params=params, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë¦¬ì½œ ì •ë³´ íŒŒì‹±
            recall_items = soup.select('.recall-item, .list-item, tr')  # ì‹¤ì œ ì…€ë ‰í„°ë¡œ ë³€ê²½ í•„ìš”
            
            for item in recall_items:
                try:
                    # ë¦¬ì½œ ì œëª©
                    title_elem = item.select_one('.title, .subject, td:nth-child(2)')
                    title = title_elem.text.strip() if title_elem else 'ì œëª© ì—†ìŒ'
                    
                    # ë¦¬ì½œ ë‚ ì§œ
                    date_elem = item.select_one('.date, .regdate, td:nth-child(4)')
                    recall_date = self._parse_date(date_elem.text.strip()) if date_elem else None
                    
                    # ë¦¬ì½œ ì‚¬ìœ 
                    reason_elem = item.select_one('.reason, .content, td:nth-child(3)')
                    reason = reason_elem.text.strip() if reason_elem else 'ì‚¬ìœ  ì—†ìŒ'
                    
                    # ì˜í–¥ ëŒ€ìˆ˜
                    units_elem = item.select_one('.units, .count, td:nth-child(5)')
                    affected_units = self._extract_number(units_elem.text) if units_elem else 0
                    
                    # ì‹¬ê°ë„ íŒë‹¨
                    severity = self._determine_severity(title, reason)
                    
                    recall_data.append({
                        'manufacturer': manufacturer,
                        'model_name': model_name,
                        'recall_date': recall_date,
                        'recall_title': title,
                        'recall_reason': reason,
                        'affected_units': affected_units,
                        'severity_level': severity
                    })
                    
                except Exception as e:
                    logger.debug(f"ê°œë³„ ë¦¬ì½œ í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
                    
            logger.info(f"âœ… {model_name}: {len(recall_data)}ê±´ì˜ ë¦¬ì½œ ì •ë³´ ìˆ˜ì§‘")
            
        except Exception as e:
            logger.error(f"ë¦¬ì½œ ì •ë³´ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            
        return recall_data
        
    def _parse_date(self, date_str):
        """ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±"""
        try:
            # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
            for fmt in ['%Y-%m-%d', '%Y.%m.%d', '%Y/%m/%d', '%Yë…„ %mì›” %dì¼']:
                try:
                    return datetime.strptime(date_str, fmt).date()
                except:
                    continue
            return None
        except:
            return None
            
    def _extract_number(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ"""
        import re
        numbers = re.findall(r'\d+', text.replace(',', ''))
        return int(numbers[0]) if numbers else 0
        
    def _determine_severity(self, title, reason):
        """ë¦¬ì½œ ì‹¬ê°ë„ íŒë‹¨"""
        critical_keywords = ['í™”ì¬', 'ì—”ì§„', 'ë¸Œë ˆì´í¬', 'ì—ì–´ë°±', 'ì¡°í–¥']
        severe_keywords = ['ë³€ì†ê¸°', 'ì—°ë£Œ', 'ë°°ì¶œê°€ìŠ¤', 'ì „ê¸°']
        moderate_keywords = ['ëˆ„ìˆ˜', 'ì†ŒìŒ', 'ì§„ë™', 'ì„¼ì„œ']
        
        text = (title + ' ' + reason).lower()
        
        if any(keyword in text for keyword in critical_keywords):
            return 'ë§¤ìš°ì‹¬ê°'
        elif any(keyword in text for keyword in severe_keywords):
            return 'ì‹¬ê°'
        elif any(keyword in text for keyword in moderate_keywords):
            return 'ë³´í†µ'
        else:
            return 'ê²½ë¯¸'
            
    def crawl_and_save(self, car_list):
        """ì°¨ëŸ‰ ëª©ë¡ì˜ ë¦¬ì½œ ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ê³  DBì— ì €ì¥"""
        db_helper.update_crawling_log('recall', 'ì‹œì‘')
        total_collected = 0
        
        try:
            for car in car_list:
                manufacturer = car['manufacturer']
                model_name = car['model_name']
                
                # ëª¨ë¸ ID ì¡°íšŒ
                model_id = db_helper.get_car_model_id(manufacturer, model_name)
                if not model_id:
                    logger.warning(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {manufacturer} {model_name}")
                    continue
                    
                # ë¦¬ì½œ ì •ë³´ ìˆ˜ì§‘
                recall_data = self.search_recall_info(manufacturer, model_name)
                
                # DBì— ì €ì¥
                for recall in recall_data:
                    db_helper.insert_recall_info(
                        model_id=model_id,
                        recall_date=recall['recall_date'],
                        recall_title=recall['recall_title'],
                        recall_reason=recall['recall_reason'],
                        affected_units=recall['affected_units'],
                        severity_level=recall['severity_level']
                    )
                    
                total_collected += len(recall_data)
                time.sleep(self.delay)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
                
            db_helper.update_crawling_log('recall', 'ì™„ë£Œ', total_collected)
            logger.info(f"ğŸ‰ ë¦¬ì½œ ì •ë³´ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {total_collected}ê±´ ìˆ˜ì§‘")
            
        except Exception as e:
            db_helper.update_crawling_log('recall', 'ì‹¤íŒ¨', total_collected, str(e))
            logger.error(f"ë¦¬ì½œ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    crawler = RecallCrawler()
    
    # í…ŒìŠ¤íŠ¸í•  ì°¨ëŸ‰ ëª©ë¡
    test_cars = [
        {'manufacturer': 'í˜„ëŒ€', 'model_name': 'ê·¸ëœì € IG'},
        {'manufacturer': 'í˜„ëŒ€', 'model_name': 'ì˜ë‚˜íƒ€ DN8'},
        {'manufacturer': 'ê¸°ì•„', 'model_name': 'K5 DL3'},
    ]
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    # crawler.crawl_and_save(test_cars)
    
    print("ë¦¬ì½œ ì •ë³´ í¬ë¡¤ëŸ¬ ì¤€ë¹„ ì™„ë£Œ!")
